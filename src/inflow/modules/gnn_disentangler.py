

import os, sys
import numpy as np
import torch
import torch.nn as nn
from . import gnn
import torch_geometric as pyg

class GNNDisentangler(nn.Module):
    def __init__(self, kwargs_genmodel, str_mode_normalizex:str, maxsize_subgraph, dict_general_args, str_mode_headxint_headxspl_headboth, gnn_list_dim_hidden, kwargs_sageconv, clipval_cov_noncentralnodes:float):
        '''
        :param maxsize_subgraph: the max size of the subgraph returned by pyg's NeighLoader.
        :param str_mode_normalizex: in ['counts', 'logp1'], whether the GNN works on counts or log1p
        :param dict_general_args: a dict containint
            - num_celltypes
            - flag_use_int_u
            - flag_use_spl_u
        :param str_mode_headxint_headxspl_headboth:
            - headxint: means that only muxint is generated by the head, and `muxspl =  x_cnt - muxint`
            - headxspl: means that only muxspl is generated by the head, and `muxint =  x_cnt - muxspl`
            - headboth: means muxint and muxspl are create by two different heads, in which case p(x|x_int+x_spl) must be included in logp(.)
        :param clipval_cov_noncentralnodes: the value by which the covariance for non-central nodes is clipped.
        '''
        super(GNNDisentangler, self).__init__()
        self.kwargs_genmodel = kwargs_genmodel
        self.num_celltypes = dict_general_args['num_celltypes']
        self.flag_use_int_u = dict_general_args['flag_use_int_u']
        self.flag_use_spl_u = dict_general_args['flag_use_spl_u']
        self.str_mode_headxint_headxspl_headboth = str_mode_headxint_headxspl_headboth
        self.gnn_list_dim_hidden = gnn_list_dim_hidden
        self.kwargs_sageconv = kwargs_sageconv
        self.str_mode_normalizex = str_mode_normalizex
        self.clipval_cov_noncentralnodes = clipval_cov_noncentralnodes

        dim_gnnin = kwargs_genmodel['dict_varname_to_dim']['x']

        if self.flag_use_int_u:
            dim_gnnin += kwargs_genmodel['dict_varname_to_dim']['u_int']
        if self.flag_use_spl_u:
            dim_gnnin += kwargs_genmodel['dict_varname_to_dim']['u_spl']

        '''
        self.module_em1 = SubgraphEmbeddingImpAndDisengl(**kwargs_em1)
        self.module_tf1 = Padder(
            Linformer(**{
                **{'input_size':maxsize_subgraph, 'channels':dim_tf1},
                **kwargs_tformer1
            })
        )
        '''

        self.module_gnn_backbone = gnn.SAGE(
            dim_input=dim_gnnin,
            dim_output=100,  # TODO:TUNE 100
            list_dim_hidden=self.gnn_list_dim_hidden,
            kwargs_sageconv=self.kwargs_sageconv
        )

        # assert that the GNN backbone has as many hops as the generative model
        cnt_sage_conv = 0
        for ch in self.module_gnn_backbone.list_modules:
            if isinstance(ch, pyg.nn.SAGEConv) or isinstance(ch, gnn.SageConvAndActivation):
                cnt_sage_conv += 1

        assert (
            cnt_sage_conv == self.kwargs_genmodel['kwargs_theta_aggr']['num_hops']
        )


        if self.str_mode_headxint_headxspl_headboth == 'headxint':
            self.module_linearhead_muxint = nn.Sequential(
                nn.ReLU(),
                nn.Linear(
                    self.module_gnn_backbone.dim_output,
                    kwargs_genmodel['dict_varname_to_dim']['x']
                )
            )
            self.module_linearhead_muxspl = None
        elif self.str_mode_headxint_headxspl_headboth == 'headxspl':
            self.module_linearhead_muxint = None
            self.module_linearhead_muxspl = nn.Sequential(
                nn.ReLU(),
                nn.Linear(
                    self.module_gnn_backbone.dim_output,
                    kwargs_genmodel['dict_varname_to_dim']['x']
                )
            )
        elif self.str_mode_headxint_headxspl_headboth == 'headboth':
            self.module_linearhead_muxint = nn.Sequential(
                nn.ReLU(),
                nn.Linear(
                    self.module_gnn_backbone.dim_output,
                    kwargs_genmodel['dict_varname_to_dim']['x']
                )
            )
            self.module_linearhead_muxspl = nn.Sequential(
                nn.ReLU(),
                nn.Linear(
                    self.module_gnn_backbone.dim_output,
                    kwargs_genmodel['dict_varname_to_dim']['x']
                )
            )
        else:
            raise Exception(
                "Uknown value {} for str_mode_headxint_headxspl_headboth.".format(self.str_mode_headxint_headxspl_headboth)
            )

        self.module_linearhead_sigmaxint = nn.Sequential(
            nn.LeakyReLU(),
            nn.Linear(self.module_gnn_backbone.dim_output, dict_general_args['num_genes'])
        )
        self.module_linearhead_sigmaxspl = nn.Sequential(
            nn.LeakyReLU(),
            nn.Linear(self.module_gnn_backbone.dim_output, dict_general_args['num_genes'])
        )
        self._check_args()

    def _check_args(self):
        assert (
            self.str_mode_headxint_headxspl_headboth in ['headxint', 'headxspl', 'headboth']
        )
        assert (
            self.str_mode_normalizex in ['counts', 'log1p']
        )

    def forward(self, batch, prob_maskknowngenes:float, ten_xy_absolute:torch.Tensor):
        '''
        :param batch:
        :param prob_maskknowngenes: must be zero for `Disentangler`, this arg is kept for consistency.
        :param ten_xy_absolute:
        :return:
        '''

        with torch.no_grad():
            assert(prob_maskknowngenes == 0.0)
            x_log1p = torch.log(
                1.0 + batch.x.to_dense()  # TODO: how to make sure that batch.x contains the count data ???
            ).to(ten_xy_absolute.device)
            x_cnt = batch.x.to_dense().to(ten_xy_absolute.device).detach() + 0.0

            # make GNN's input ===
            ten_u_int = batch.y[:, 0:batch.INFLOWMETAINF['dim_u_int']].to(ten_xy_absolute.device) if (self.flag_use_int_u) else None
            ten_u_spl = batch.y[:, batch.INFLOWMETAINF['dim_u_int']::].to(ten_xy_absolute.device) if (self.flag_use_spl_u) else None

            x_gnn_input = x_cnt if (self.str_mode_normalizex == 'counts') else x_log1p
            if ten_u_int is not None:
                x_gnn_input = torch.cat([x_gnn_input, ten_u_int], 1)
            if ten_u_spl is not None:
                x_gnn_input = torch.cat([x_gnn_input, ten_u_spl], 1)


        output_gnn_backbone = self.module_gnn_backbone(
            x_gnn_input,
            batch.edge_index.to(ten_xy_absolute.device)
        )



        if torch.any(torch.isnan(output_gnn_backbone)):
            print(
                "Does it happen on central nodes? {}".format(
                    torch.any(
                        torch.isnan(output_gnn_backbone[0:batch.batch_size])
                    )
                )
            )
            print(
                "Are any of GNN parameters NaN? {}".format(
                    torch.stack([torch.isnan(p).any() for p in self.module_gnn_backbone.parameters()]).any()
                )
            )
            print("NAN was found in GNN's output, probably because a cell didn't have a neighbour in mini-batch???")
            output_gnn_backbone = torch.nan_to_num(output_gnn_backbone)



        # assert (not torch.any(ten_manually_masked))
        loss_imputex = None
        ten_out_imputer = 0.0



        # compute muxint, muxspl
        with torch.no_grad():
            oneon_x_nonzero = (x_cnt > 0.0) + 0  # [N, num_genes]

        if self.str_mode_headxint_headxspl_headboth == 'headxint':
            muxint = torch.clamp(
                self.module_linearhead_muxint(output_gnn_backbone) * oneon_x_nonzero,
                min=torch.tensor([0.0001], device=ten_xy_absolute.device),  # TODO: maybe tune?
                max=x_cnt
            )  # [N, num_genes]
            muxspl = x_cnt - muxint  # [N, num_genes]
        elif self.str_mode_headxint_headxspl_headboth == 'headxspl':
            muxspl = torch.clamp(
                self.module_linearhead_muxspl(output_gnn_backbone) * oneon_x_nonzero,
                min=torch.tensor([0.0001], device=ten_xy_absolute.device),  # TODO: maybe tune?
                max=x_cnt
            )  # [N, num_genes]
            muxint = x_cnt - muxspl  # [N, num_genes]
        elif self.str_mode_headxint_headxspl_headboth == 'headboth':
            muxint = torch.clamp(
                self.module_linearhead_muxint(output_gnn_backbone) * oneon_x_nonzero,
                min=torch.tensor([0.0001], device=ten_xy_absolute.device),  # TODO: maybe tune?
                max=x_cnt
            )  # [N, num_genes]
            muxspl = torch.clamp(
                self.module_linearhead_muxspl(output_gnn_backbone) * oneon_x_nonzero,
                min=torch.tensor([0.0001], device=ten_xy_absolute.device),  # TODO: maybe tune?
                max=x_cnt
            )  # [N, num_genes]
        else:
            raise Exception(
                "Uknown value {} for str_mode_headxint_headxspl_headboth.".format(self.str_mode_headxint_headxspl_headboth)
            )


        # compute sigmaxint, sigmaxspl
        sigmaxint_raw = torch.clamp(
            torch.exp(
                self.module_linearhead_sigmaxint(output_gnn_backbone)
            ),
            min=0.001,  # TODO: maybe tune?
            max=4.0
        )  # [N, num_genes]
        sigmaxspl_raw = torch.clamp(
            torch.exp(
                self.module_linearhead_sigmaxspl(output_gnn_backbone)
            ),
            min=0.001,  # TODO: maybe tune?
            max=4.0
        )  # [N, num_genes]

        '''
        The below part puts a lower bound on the variance on non-central nodes.
        This is done to solve the conceptual issue of getting GNN output on non-central nodes.
        '''
        sigmaxint = torch.concat(
            [sigmaxint_raw[:,0:batch.batch_size]+0.0, torch.clamp(sigmaxint_raw[:,batch.batch_size::]+0.0, min=self.clipval_cov_noncentralnodes, max=4.0)],
            1
        ).sqrt()
        sigmaxspl = torch.concat(
            [sigmaxspl_raw[:, 0:batch.batch_size]+0.0, torch.clamp(sigmaxspl_raw[:, batch.batch_size::]+0.0, min=self.clipval_cov_noncentralnodes, max=4.0)],
            1
        ).sqrt()



        return dict(
            muxint=muxint,
            muxspl=muxspl,
            sigmaxint=sigmaxint,
            sigmaxspl=sigmaxspl,
            ten_out_imputer=ten_out_imputer + 0.0,
            loss_imputex=loss_imputex
        )



